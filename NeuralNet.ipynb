{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NeuralNet.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "oiiFQmZePb-E"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "class neural_network:\n",
        "    def __init__(self, num_layers, num_nodes, activation_function, cost_function):\n",
        "        self.num_layers = num_layers\n",
        "        self.num_nodes = num_nodes\n",
        "        self.layers = []\n",
        "        self.cost_function = cost_function\n",
        "\n",
        "        for i in range(num_layers):\n",
        "            if i != num_layers - 1:\n",
        "                layer_i = layer(num_nodes[i], num_nodes[i + 1], activation_function[i])\n",
        "            else:\n",
        "                layer_i = layer(num_nodes[i], 0, activation_function[i])\n",
        "            self.layers.append(layer_i)\n",
        "\n",
        "    def train(self, batch_size, inputs, labels, num_epochs, learning_rate, filename):\n",
        "        self.batch_size = batch_size\n",
        "        self.learning_rate = learning_rate\n",
        "        for j in range(num_epochs):\n",
        "            i = 0\n",
        "            print(\"== EPOCH: \", j, \" ==\")\n",
        "            while i + batch_size != len(inputs):\n",
        "                self.error = 0\n",
        "                self.forward_pass(inputs[i : i + batch_size])\n",
        "                self.calculate_error(labels[i : i + batch_size])\n",
        "                self.back_pass(labels[i : i + batch_size])\n",
        "                i += batch_size\n",
        "            print(\"Error: \", self.error)\n",
        "        dill.dump_session(filename)\n",
        "\n",
        "    def forward_pass(self, inputs):\n",
        "        self.layers[0].activations = inputs\n",
        "        for i in range(self.num_layers - 1):\n",
        "            self.layers[i].add_bias(\n",
        "                self.batch_size, self.layers[i + 1].num_nodes_in_layer\n",
        "            )\n",
        "            temp = np.add(\n",
        "                np.matmul(self.layers[i].activations, self.layers[i].weights_for_layer),\n",
        "                self.layers[i].bias_for_layer,\n",
        "            )\n",
        "            if self.layers[i + 1].activation_function == \"sigmoid\":\n",
        "                self.layers[i + 1].activations = self.sigmoid(temp)\n",
        "            elif self.layers[i + 1].activation_function == \"softmax\":\n",
        "                self.layers[i + 1].activations = self.softmax(temp)\n",
        "            elif self.layers[i + 1].activation_function == \"relu\":\n",
        "                self.layers[i + 1].activations = self.relu(temp)\n",
        "            elif self.layers[i + 1].activation_function == \"tanh\":\n",
        "                self.layers[i + 1].activations = self.tanh(temp)\n",
        "            else:\n",
        "                self.layers[i + 1].activations = temp\n",
        "\n",
        "    def relu(self, layer):\n",
        "        layer[layer < 0] = 0\n",
        "        return layer\n",
        "\n",
        "    def softmax(self, layer):\n",
        "        exp = np.exp(layer)\n",
        "        return exp / np.sum(exp, axis=1, keepdims=True)\n",
        "\n",
        "    def sigmoid(self, layer):\n",
        "        return np.divide(1, np.add(1, np.exp(layer)))\n",
        "\n",
        "    def tanh(self, layer):\n",
        "        return np.tanh(layer)\n",
        "\n",
        "    def calculate_error(self, labels):\n",
        "        if len(labels[0]) != self.layers[self.num_layers - 1].num_nodes_in_layer:\n",
        "            print(\"Error: Label is not of the same shape as output layer.\")\n",
        "            print(\"Label: \", len(labels), \" : \", len(labels[0]))\n",
        "            print(\n",
        "                \"Out: \",\n",
        "                len(self.layers[self.num_layers - 1].activations),\n",
        "                \" : \",\n",
        "                len(self.layers[self.num_layers - 1].activations[0]),\n",
        "            )\n",
        "            return\n",
        "\n",
        "        if self.cost_function == \"mean_squared\":\n",
        "            self.error = np.mean(\n",
        "                np.divide(\n",
        "                    np.square(\n",
        "                        np.subtract(\n",
        "                            labels, self.layers[self.num_layers - 1].activations\n",
        "                        )\n",
        "                    ),\n",
        "                    2,\n",
        "                )\n",
        "            )\n",
        "        elif self.cost_function == \"cross_entropy\":\n",
        "            self.error = np.negative(\n",
        "                np.sum(\n",
        "                    np.multiply(\n",
        "                        labels, np.log(self.layers[self.num_layers - 1].activations)\n",
        "                    )\n",
        "                )\n",
        "            )\n",
        "\n",
        "    def back_pass(self, labels):\n",
        "        # if self.cost_function == \"cross_entropy\" and self.layers[self.num_layers-1].activation_function == \"softmax\":\n",
        "        targets = labels\n",
        "        i = self.num_layers - 1\n",
        "        y = self.layers[i].activations\n",
        "        deltaw = np.matmul(\n",
        "            np.asarray(self.layers[i - 1].activations).T,\n",
        "            np.multiply(y, np.multiply(1 - y, targets - y)),\n",
        "        )\n",
        "        new_weights = self.layers[i - 1].weights_for_layer - self.learning_rate * deltaw\n",
        "        for i in range(i - 1, 0, -1):\n",
        "            y = self.layers[i].activations\n",
        "            deltaw = np.matmul(\n",
        "                np.asarray(self.layers[i - 1].activations).T,\n",
        "                np.multiply(\n",
        "                    y,\n",
        "                    np.multiply(\n",
        "                        1 - y,\n",
        "                        np.sum(\n",
        "                            np.multiply(new_weights, self.layers[i].weights_for_layer),\n",
        "                            axis=1,\n",
        "                        ).T,\n",
        "                    ),\n",
        "                ),\n",
        "            )\n",
        "            self.layers[i].weights_for_layer = new_weights\n",
        "            new_weights = (\n",
        "                self.layers[i - 1].weights_for_layer - self.learning_rate * deltaw\n",
        "            )\n",
        "        self.layers[0].weights_for_layer = new_weights\n",
        "\n",
        "    def predict(self, filename, input):\n",
        "        dill.load_session(filename)\n",
        "        self.batch_size = 1\n",
        "        self.forward_pass(input)\n",
        "        a = self.layers[self.num_layers - 1].activations\n",
        "        a[np.where(a == np.max(a))] = 0.9\n",
        "        a[np.where(a != np.max(a))] = 0.1\n",
        "        return a\n",
        "\n",
        "    def check_accuracy(self, filename, inputs, labels):\n",
        "        dill.load_session(filename)\n",
        "        self.batch_size = len(inputs)\n",
        "        self.forward_pass(inputs)\n",
        "        a = self.layers[self.num_layers - 1].activations\n",
        "        num_classes = 10\n",
        "        targets = np.array([a]).reshape(-1)\n",
        "        a = np.asarray(a)\n",
        "        one_hot_labels = np.eye(num_classes)[a.astype(int)]\n",
        "        total = 0\n",
        "        correct = 0\n",
        "        for i in range(len(a)):\n",
        "            total += 1\n",
        "            if np.equal(one_hot_labels[i], labels[i]).all():\n",
        "                correct += 1\n",
        "        print(\"Accuracy: \", correct * 100 / total)\n",
        "\n",
        "    def load_model(self, filename):\n",
        "        dill.load_session(filename)\n",
        "\n",
        "\n",
        "class layer:\n",
        "    def __init__(\n",
        "        self, num_nodes_in_layer, num_nodes_in_next_layer, activation_function\n",
        "    ):\n",
        "        self.num_nodes_in_layer = num_nodes_in_layer\n",
        "        self.activation_function = activation_function\n",
        "        self.activations = np.zeros([num_nodes_in_layer, 1])\n",
        "        if num_nodes_in_next_layer != 0:\n",
        "            self.weights_for_layer = np.random.normal(\n",
        "                0, 0.001, size=(num_nodes_in_layer, num_nodes_in_next_layer)\n",
        "            )\n",
        "        else:\n",
        "            self.weights_for_layer = None\n",
        "            self.bias_for_layer = None\n",
        "\n",
        "    def add_bias(self, batch_size, num_nodes_in_next_layer):\n",
        "        if num_nodes_in_next_layer != 0:\n",
        "            self.bias_for_layer = np.random.normal(\n",
        "                0, 0.01, size=(batch_size, num_nodes_in_next_layer)\n",
        "            )"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}